{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Character-Level LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttolofari/Character-Level-LSTM/blob/master/Character_Level_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6PxEye-xpaaQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Character Level LSTM**"
      ]
    },
    {
      "metadata": {
        "id": "GXdX9wKLHJ_I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "08ufWileHaaJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Opening the text file in a read mode\n",
        "\n",
        "with open('anna.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AdzcgFjmHx4H",
        "colab_type": "code",
        "outputId": "c3099651-e717-4240-b7c5-061dff1ba7f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Looking at the first 100 characters in the file\n",
        "\n",
        "text[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "YQ7hzpI3IYAn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Tokenization of the characters\n",
        "\n",
        "# Using dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier for the network\n",
        "\n",
        "# int2char, this maps integers to characters\n",
        "# char2int, this maps characters to unique integers\n",
        "\n",
        "chars = tuple(set(text)) # Creating a set of all the characters in the text\n",
        "int2char = dict(enumerate(chars))\n",
        "\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "\n",
        "# Encoding the text\n",
        "\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HjJ1zCT9JIRH",
        "colab_type": "code",
        "outputId": "c0585f8d-d2ec-48b6-a2ca-f5d5a7a6e660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# Displaying the output of the encoded characters\n",
        "\n",
        "encoded[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([32, 33, 34, 29, 21, 26, 67, 78,  9, 71, 71, 71,  8, 34, 29, 29, 73,\n",
              "       78, 50, 34, 35, 24, 69, 24, 26, 31, 78, 34, 67, 26, 78, 34, 69, 69,\n",
              "       78, 34, 69, 24, 63, 26, 56, 78, 26, 65, 26, 67, 73, 78, 62, 80, 33,\n",
              "       34, 29, 29, 73, 78, 50, 34, 35, 24, 69, 73, 78, 24, 31, 78, 62, 80,\n",
              "       33, 34, 29, 29, 73, 78, 24, 80, 78, 24, 21, 31, 78, 39, 40, 80, 71,\n",
              "       40, 34, 73, 64, 71, 71, 10, 65, 26, 67, 73, 21, 33, 24, 80])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "tg-HsC7xKXpX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preprocessing the data\n",
        "# The LSTM expects an input that is one-hot encoded meaning that each character is converted into an integer and then converted into a column vector where only\n",
        "#its corresponding integer index will have the value of 1 and the rest vector will be filled with 0's. \n",
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialixe the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype = np.float32)\n",
        "    \n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get it back to the original array\n",
        "    \n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KbP_O_RBMI0R",
        "colab_type": "code",
        "outputId": "cd6f4d62-8b02-4079-ff67-3db8008e1808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Check that the function works as expected\n",
        "\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0tZE71FjMQO3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Making mini-batches\n",
        "\n",
        "def get_batches(arr, batch_size, seq_length):\n",
        "  \n",
        "    # Get the number of batches we can make\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    \n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    \n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    \n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    \n",
        "    # Iterate through the array, one sequence at a time \n",
        "    \n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "      \n",
        "        # The features\n",
        "        x = arr[:, n : n+seq_length]\n",
        "        \n",
        "        # The targets, shifted by one\n",
        "        \n",
        "        y = np.zeros_like(x)\n",
        "        \n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-pvUFXg3W774",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Testing the implementation\n",
        "\n",
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YxjgOGO6b-A5",
        "colab_type": "code",
        "outputId": "715bdac5-671a-4b50-cb33-fb9859ba72df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "# Printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[32 33 34 29 21 26 67 78  9 71]\n",
            " [31 39 80 78 21 33 34 21 78 34]\n",
            " [26 80 16 78 39 67 78 34 78 50]\n",
            " [31 78 21 33 26 78 20 33 24 26]\n",
            " [78 31 34 40 78 33 26 67 78 21]\n",
            " [20 62 31 31 24 39 80 78 34 80]\n",
            " [78 61 80 80 34 78 33 34 16 78]\n",
            " [57 14 69 39 80 31 63 73 64 78]]\n",
            "\n",
            "y\n",
            " [[33 34 29 21 26 67 78  9 71 71]\n",
            " [39 80 78 21 33 34 21 78 34 21]\n",
            " [80 16 78 39 67 78 34 78 50 39]\n",
            " [78 21 33 26 78 20 33 24 26 50]\n",
            " [31 34 40 78 33 26 67 78 21 26]\n",
            " [62 31 31 24 39 80 78 34 80 16]\n",
            " [61 80 80 34 78 33 34 16 78 31]\n",
            " [14 69 39 80 31 63 73 64 78 76]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LrZFHG9jcpLp",
        "colab_type": "code",
        "outputId": "17b0ef90-47ca-4dcc-e6d1-a6a8ac4febaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Defining the model\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU')\n",
        "else:\n",
        "    print('No GPU available. training on CPU, consider making n_epochs very small.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HRBcUNKkdR5a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "  \n",
        "    def __init__(self, tokens, n_hidden = 256, n_layers = 2, drop_prob = 0.5, lr = 0.001):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        \n",
        "        # Creating character dictionaries\n",
        "        \n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        # Defining the LSTM layer\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout = drop_prob, batch_first = True)\n",
        "        \n",
        "        # Defining the Dropout\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        # Definng the final fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "        \n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "      \n",
        "        # Getting the output and new hidden state from the LSTM\n",
        "        \n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        # Pass through the dropout layer\n",
        "        \n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LST outputs using view\n",
        "        \n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        # Put through FC layer\n",
        "        \n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "      \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "      \n",
        "        '''Initializes hidden state'''\n",
        "        \n",
        "        # Creates two new tensors with sizes n_layers X batch_size X n_hidden, \n",
        "        # initialize to zero, for hidden state and cell state of LSTM\n",
        "        \n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KFe5hKnp15cM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(net, data, epochs = 10, batch_size = 10, seq_length = 50, lr = 0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "  \n",
        "    net.train()\n",
        "    opt = torch.optim.Adam(net.parameters(), lr= lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Create training and validation data\n",
        "    \n",
        "    val_idx = int(len(data)*(1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if (train_on_gpu):\n",
        "        net.cuda()\n",
        "        \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    \n",
        "    for e in range(epochs):\n",
        "      \n",
        "        # Initialize hidden state\n",
        "        \n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One_hot encode our data and make them torch tensors\n",
        "            \n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if (train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            \n",
        "            # Creating a new variable for the hidden state to avoid backpropagation through the entire history\n",
        "            \n",
        "            h = tuple([each.data for each in h])\n",
        "            \n",
        "            # Zero gradient\n",
        "            \n",
        "            net.zero_grad()\n",
        "            \n",
        "            # Output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # Calculate the loss\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            # This is to help prevent exploding gradient problem\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # Loss stat\n",
        "            \n",
        "            if counter % print_every == 0:\n",
        "              \n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One_hot encode our data and make them torch tensors\n",
        "                    \n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating a new variable for the hidden state to avoid backpropagation through the entire history\n",
        "                    \n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if (train_on_gpu):\n",
        "                        inputs, targets = torch.Tensor(inputs).cuda(), torch.Tensor(targets).cuda().long()\n",
        "                    \n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
        "                    val_losses.append(val_loss.item())\n",
        "                    \n",
        "                net.train() # reset to train mode after iterationg through validation data    \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs), \n",
        "                         \"Steps: {}...\".format(counter), \n",
        "                         \"Loss: {:.4f}...\".format(loss.item()), \n",
        "                         \"Val Loss {:.4f}\".format(np.mean(val_losses)))\n",
        "              \n",
        "                    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4grDV03H25aq",
        "colab_type": "code",
        "outputId": "9ba829b1-5572-4a9b-fd5f-494b9be11dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M1vO7Bul9AAr",
        "colab_type": "code",
        "outputId": "11147942-4c26-404d-ba30-bebb1d520871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4743
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs =  20 # start small if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Steps: 10... Loss: 3.2976... Val Loss 3.2488\n",
            "Epoch: 1/20... Steps: 20... Loss: 3.1530... Val Loss 3.1438\n",
            "Epoch: 1/20... Steps: 30... Loss: 3.1392... Val Loss 3.1268\n",
            "Epoch: 1/20... Steps: 40... Loss: 3.1152... Val Loss 3.1195\n",
            "Epoch: 1/20... Steps: 50... Loss: 3.1443... Val Loss 3.1175\n",
            "Epoch: 1/20... Steps: 60... Loss: 3.1208... Val Loss 3.1160\n",
            "Epoch: 1/20... Steps: 70... Loss: 3.1077... Val Loss 3.1148\n",
            "Epoch: 1/20... Steps: 80... Loss: 3.1251... Val Loss 3.1126\n",
            "Epoch: 1/20... Steps: 90... Loss: 3.1232... Val Loss 3.1063\n",
            "Epoch: 1/20... Steps: 100... Loss: 3.1033... Val Loss 3.0929\n",
            "Epoch: 1/20... Steps: 110... Loss: 3.0837... Val Loss 3.0710\n",
            "Epoch: 1/20... Steps: 120... Loss: 3.0123... Val Loss 3.0123\n",
            "Epoch: 1/20... Steps: 130... Loss: 2.9403... Val Loss 2.9098\n",
            "Epoch: 2/20... Steps: 140... Loss: 2.8083... Val Loss 2.7605\n",
            "Epoch: 2/20... Steps: 150... Loss: 2.7335... Val Loss 2.6815\n",
            "Epoch: 2/20... Steps: 160... Loss: 2.6126... Val Loss 2.5767\n",
            "Epoch: 2/20... Steps: 170... Loss: 2.5370... Val Loss 2.5204\n",
            "Epoch: 2/20... Steps: 180... Loss: 2.5007... Val Loss 2.4760\n",
            "Epoch: 2/20... Steps: 190... Loss: 2.4484... Val Loss 2.4410\n",
            "Epoch: 2/20... Steps: 200... Loss: 2.4366... Val Loss 2.4029\n",
            "Epoch: 2/20... Steps: 210... Loss: 2.3996... Val Loss 2.3740\n",
            "Epoch: 2/20... Steps: 220... Loss: 2.3562... Val Loss 2.3447\n",
            "Epoch: 2/20... Steps: 230... Loss: 2.3537... Val Loss 2.3460\n",
            "Epoch: 2/20... Steps: 240... Loss: 2.3362... Val Loss 2.2954\n",
            "Epoch: 2/20... Steps: 250... Loss: 2.2595... Val Loss 2.2638\n",
            "Epoch: 2/20... Steps: 260... Loss: 2.2399... Val Loss 2.2386\n",
            "Epoch: 2/20... Steps: 270... Loss: 2.2403... Val Loss 2.2071\n",
            "Epoch: 3/20... Steps: 280... Loss: 2.2376... Val Loss 2.1850\n",
            "Epoch: 3/20... Steps: 290... Loss: 2.1905... Val Loss 2.1554\n",
            "Epoch: 3/20... Steps: 300... Loss: 2.1728... Val Loss 2.1330\n",
            "Epoch: 3/20... Steps: 310... Loss: 2.1396... Val Loss 2.1113\n",
            "Epoch: 3/20... Steps: 320... Loss: 2.1110... Val Loss 2.0903\n",
            "Epoch: 3/20... Steps: 330... Loss: 2.0787... Val Loss 2.0731\n",
            "Epoch: 3/20... Steps: 340... Loss: 2.0881... Val Loss 2.0557\n",
            "Epoch: 3/20... Steps: 350... Loss: 2.0700... Val Loss 2.0326\n",
            "Epoch: 3/20... Steps: 360... Loss: 2.0081... Val Loss 2.0135\n",
            "Epoch: 3/20... Steps: 370... Loss: 2.0293... Val Loss 1.9911\n",
            "Epoch: 3/20... Steps: 380... Loss: 2.0100... Val Loss 1.9743\n",
            "Epoch: 3/20... Steps: 390... Loss: 1.9801... Val Loss 1.9624\n",
            "Epoch: 3/20... Steps: 400... Loss: 1.9533... Val Loss 1.9426\n",
            "Epoch: 3/20... Steps: 410... Loss: 1.9552... Val Loss 1.9276\n",
            "Epoch: 4/20... Steps: 420... Loss: 1.9485... Val Loss 1.9111\n",
            "Epoch: 4/20... Steps: 430... Loss: 1.9325... Val Loss 1.8972\n",
            "Epoch: 4/20... Steps: 440... Loss: 1.9095... Val Loss 1.8862\n",
            "Epoch: 4/20... Steps: 450... Loss: 1.8669... Val Loss 1.8720\n",
            "Epoch: 4/20... Steps: 460... Loss: 1.8484... Val Loss 1.8572\n",
            "Epoch: 4/20... Steps: 470... Loss: 1.8781... Val Loss 1.8451\n",
            "Epoch: 4/20... Steps: 480... Loss: 1.8508... Val Loss 1.8327\n",
            "Epoch: 4/20... Steps: 490... Loss: 1.8630... Val Loss 1.8209\n",
            "Epoch: 4/20... Steps: 500... Loss: 1.8592... Val Loss 1.8095\n",
            "Epoch: 4/20... Steps: 510... Loss: 1.8156... Val Loss 1.8017\n",
            "Epoch: 4/20... Steps: 520... Loss: 1.8363... Val Loss 1.7883\n",
            "Epoch: 4/20... Steps: 530... Loss: 1.7953... Val Loss 1.7794\n",
            "Epoch: 4/20... Steps: 540... Loss: 1.7555... Val Loss 1.7761\n",
            "Epoch: 4/20... Steps: 550... Loss: 1.8056... Val Loss 1.7563\n",
            "Epoch: 5/20... Steps: 560... Loss: 1.7729... Val Loss 1.7457\n",
            "Epoch: 5/20... Steps: 570... Loss: 1.7588... Val Loss 1.7374\n",
            "Epoch: 5/20... Steps: 580... Loss: 1.7334... Val Loss 1.7293\n",
            "Epoch: 5/20... Steps: 590... Loss: 1.7361... Val Loss 1.7219\n",
            "Epoch: 5/20... Steps: 600... Loss: 1.7236... Val Loss 1.7143\n",
            "Epoch: 5/20... Steps: 610... Loss: 1.7126... Val Loss 1.7016\n",
            "Epoch: 5/20... Steps: 620... Loss: 1.7146... Val Loss 1.6963\n",
            "Epoch: 5/20... Steps: 630... Loss: 1.7285... Val Loss 1.6925\n",
            "Epoch: 5/20... Steps: 640... Loss: 1.6968... Val Loss 1.6830\n",
            "Epoch: 5/20... Steps: 650... Loss: 1.6773... Val Loss 1.6741\n",
            "Epoch: 5/20... Steps: 660... Loss: 1.6649... Val Loss 1.6650\n",
            "Epoch: 5/20... Steps: 670... Loss: 1.6897... Val Loss 1.6629\n",
            "Epoch: 5/20... Steps: 680... Loss: 1.6861... Val Loss 1.6581\n",
            "Epoch: 5/20... Steps: 690... Loss: 1.6518... Val Loss 1.6490\n",
            "Epoch: 6/20... Steps: 700... Loss: 1.6584... Val Loss 1.6420\n",
            "Epoch: 6/20... Steps: 710... Loss: 1.6429... Val Loss 1.6344\n",
            "Epoch: 6/20... Steps: 720... Loss: 1.6312... Val Loss 1.6313\n",
            "Epoch: 6/20... Steps: 730... Loss: 1.6495... Val Loss 1.6253\n",
            "Epoch: 6/20... Steps: 740... Loss: 1.6216... Val Loss 1.6177\n",
            "Epoch: 6/20... Steps: 750... Loss: 1.6010... Val Loss 1.6151\n",
            "Epoch: 6/20... Steps: 760... Loss: 1.6395... Val Loss 1.6099\n",
            "Epoch: 6/20... Steps: 770... Loss: 1.6200... Val Loss 1.6058\n",
            "Epoch: 6/20... Steps: 780... Loss: 1.5992... Val Loss 1.5976\n",
            "Epoch: 6/20... Steps: 790... Loss: 1.5845... Val Loss 1.5943\n",
            "Epoch: 6/20... Steps: 800... Loss: 1.6018... Val Loss 1.5901\n",
            "Epoch: 6/20... Steps: 810... Loss: 1.5867... Val Loss 1.5845\n",
            "Epoch: 6/20... Steps: 820... Loss: 1.5446... Val Loss 1.5776\n",
            "Epoch: 6/20... Steps: 830... Loss: 1.5949... Val Loss 1.5727\n",
            "Epoch: 7/20... Steps: 840... Loss: 1.5533... Val Loss 1.5712\n",
            "Epoch: 7/20... Steps: 850... Loss: 1.5630... Val Loss 1.5657\n",
            "Epoch: 7/20... Steps: 860... Loss: 1.5539... Val Loss 1.5564\n",
            "Epoch: 7/20... Steps: 870... Loss: 1.5644... Val Loss 1.5542\n",
            "Epoch: 7/20... Steps: 880... Loss: 1.5628... Val Loss 1.5540\n",
            "Epoch: 7/20... Steps: 890... Loss: 1.5559... Val Loss 1.5501\n",
            "Epoch: 7/20... Steps: 900... Loss: 1.5460... Val Loss 1.5471\n",
            "Epoch: 7/20... Steps: 910... Loss: 1.5096... Val Loss 1.5380\n",
            "Epoch: 7/20... Steps: 920... Loss: 1.5276... Val Loss 1.5345\n",
            "Epoch: 7/20... Steps: 930... Loss: 1.5181... Val Loss 1.5297\n",
            "Epoch: 7/20... Steps: 940... Loss: 1.5236... Val Loss 1.5257\n",
            "Epoch: 7/20... Steps: 950... Loss: 1.5390... Val Loss 1.5220\n",
            "Epoch: 7/20... Steps: 960... Loss: 1.5303... Val Loss 1.5203\n",
            "Epoch: 7/20... Steps: 970... Loss: 1.5364... Val Loss 1.5149\n",
            "Epoch: 8/20... Steps: 980... Loss: 1.5115... Val Loss 1.5172\n",
            "Epoch: 8/20... Steps: 990... Loss: 1.5041... Val Loss 1.5080\n",
            "Epoch: 8/20... Steps: 1000... Loss: 1.4969... Val Loss 1.5049\n",
            "Epoch: 8/20... Steps: 1010... Loss: 1.5415... Val Loss 1.5040\n",
            "Epoch: 8/20... Steps: 1020... Loss: 1.5074... Val Loss 1.5032\n",
            "Epoch: 8/20... Steps: 1030... Loss: 1.4812... Val Loss 1.4987\n",
            "Epoch: 8/20... Steps: 1040... Loss: 1.5045... Val Loss 1.4939\n",
            "Epoch: 8/20... Steps: 1050... Loss: 1.4784... Val Loss 1.4891\n",
            "Epoch: 8/20... Steps: 1060... Loss: 1.4926... Val Loss 1.4860\n",
            "Epoch: 8/20... Steps: 1070... Loss: 1.4811... Val Loss 1.4878\n",
            "Epoch: 8/20... Steps: 1080... Loss: 1.4751... Val Loss 1.4818\n",
            "Epoch: 8/20... Steps: 1090... Loss: 1.4691... Val Loss 1.4782\n",
            "Epoch: 8/20... Steps: 1100... Loss: 1.4546... Val Loss 1.4735\n",
            "Epoch: 8/20... Steps: 1110... Loss: 1.4694... Val Loss 1.4691\n",
            "Epoch: 9/20... Steps: 1120... Loss: 1.4820... Val Loss 1.4687\n",
            "Epoch: 9/20... Steps: 1130... Loss: 1.4789... Val Loss 1.4655\n",
            "Epoch: 9/20... Steps: 1140... Loss: 1.4745... Val Loss 1.4628\n",
            "Epoch: 9/20... Steps: 1150... Loss: 1.4838... Val Loss 1.4611\n",
            "Epoch: 9/20... Steps: 1160... Loss: 1.4462... Val Loss 1.4599\n",
            "Epoch: 9/20... Steps: 1170... Loss: 1.4502... Val Loss 1.4582\n",
            "Epoch: 9/20... Steps: 1180... Loss: 1.4410... Val Loss 1.4580\n",
            "Epoch: 9/20... Steps: 1190... Loss: 1.4754... Val Loss 1.4535\n",
            "Epoch: 9/20... Steps: 1200... Loss: 1.4239... Val Loss 1.4480\n",
            "Epoch: 9/20... Steps: 1210... Loss: 1.4361... Val Loss 1.4488\n",
            "Epoch: 9/20... Steps: 1220... Loss: 1.4365... Val Loss 1.4465\n",
            "Epoch: 9/20... Steps: 1230... Loss: 1.4091... Val Loss 1.4444\n",
            "Epoch: 9/20... Steps: 1240... Loss: 1.4183... Val Loss 1.4435\n",
            "Epoch: 9/20... Steps: 1250... Loss: 1.4330... Val Loss 1.4392\n",
            "Epoch: 10/20... Steps: 1260... Loss: 1.4271... Val Loss 1.4375\n",
            "Epoch: 10/20... Steps: 1270... Loss: 1.4251... Val Loss 1.4331\n",
            "Epoch: 10/20... Steps: 1280... Loss: 1.4395... Val Loss 1.4283\n",
            "Epoch: 10/20... Steps: 1290... Loss: 1.4316... Val Loss 1.4321\n",
            "Epoch: 10/20... Steps: 1300... Loss: 1.4163... Val Loss 1.4299\n",
            "Epoch: 10/20... Steps: 1310... Loss: 1.4251... Val Loss 1.4285\n",
            "Epoch: 10/20... Steps: 1320... Loss: 1.3873... Val Loss 1.4289\n",
            "Epoch: 10/20... Steps: 1330... Loss: 1.3969... Val Loss 1.4261\n",
            "Epoch: 10/20... Steps: 1340... Loss: 1.3836... Val Loss 1.4184\n",
            "Epoch: 10/20... Steps: 1350... Loss: 1.3834... Val Loss 1.4189\n",
            "Epoch: 10/20... Steps: 1360... Loss: 1.3864... Val Loss 1.4206\n",
            "Epoch: 10/20... Steps: 1370... Loss: 1.3744... Val Loss 1.4156\n",
            "Epoch: 10/20... Steps: 1380... Loss: 1.4194... Val Loss 1.4126\n",
            "Epoch: 10/20... Steps: 1390... Loss: 1.4200... Val Loss 1.4098\n",
            "Epoch: 11/20... Steps: 1400... Loss: 1.4282... Val Loss 1.4101\n",
            "Epoch: 11/20... Steps: 1410... Loss: 1.4328... Val Loss 1.4108\n",
            "Epoch: 11/20... Steps: 1420... Loss: 1.4164... Val Loss 1.4044\n",
            "Epoch: 11/20... Steps: 1430... Loss: 1.3923... Val Loss 1.4099\n",
            "Epoch: 11/20... Steps: 1440... Loss: 1.4145... Val Loss 1.4036\n",
            "Epoch: 11/20... Steps: 1450... Loss: 1.3401... Val Loss 1.4026\n",
            "Epoch: 11/20... Steps: 1460... Loss: 1.3779... Val Loss 1.4004\n",
            "Epoch: 11/20... Steps: 1470... Loss: 1.3592... Val Loss 1.4044\n",
            "Epoch: 11/20... Steps: 1480... Loss: 1.3888... Val Loss 1.3942\n",
            "Epoch: 11/20... Steps: 1490... Loss: 1.3663... Val Loss 1.3956\n",
            "Epoch: 11/20... Steps: 1500... Loss: 1.3599... Val Loss 1.3961\n",
            "Epoch: 11/20... Steps: 1510... Loss: 1.3425... Val Loss 1.3924\n",
            "Epoch: 11/20... Steps: 1520... Loss: 1.3789... Val Loss 1.3921\n",
            "Epoch: 12/20... Steps: 1530... Loss: 1.4254... Val Loss 1.3897\n",
            "Epoch: 12/20... Steps: 1540... Loss: 1.3803... Val Loss 1.3902\n",
            "Epoch: 12/20... Steps: 1550... Loss: 1.3959... Val Loss 1.3875\n",
            "Epoch: 12/20... Steps: 1560... Loss: 1.3908... Val Loss 1.3834\n",
            "Epoch: 12/20... Steps: 1570... Loss: 1.3368... Val Loss 1.3889\n",
            "Epoch: 12/20... Steps: 1580... Loss: 1.3279... Val Loss 1.3860\n",
            "Epoch: 12/20... Steps: 1590... Loss: 1.3134... Val Loss 1.3856\n",
            "Epoch: 12/20... Steps: 1600... Loss: 1.3401... Val Loss 1.3821\n",
            "Epoch: 12/20... Steps: 1610... Loss: 1.3401... Val Loss 1.3849\n",
            "Epoch: 12/20... Steps: 1620... Loss: 1.3311... Val Loss 1.3754\n",
            "Epoch: 12/20... Steps: 1630... Loss: 1.3563... Val Loss 1.3767\n",
            "Epoch: 12/20... Steps: 1640... Loss: 1.3326... Val Loss 1.3809\n",
            "Epoch: 12/20... Steps: 1650... Loss: 1.3171... Val Loss 1.3749\n",
            "Epoch: 12/20... Steps: 1660... Loss: 1.3661... Val Loss 1.3731\n",
            "Epoch: 13/20... Steps: 1670... Loss: 1.3365... Val Loss 1.3678\n",
            "Epoch: 13/20... Steps: 1680... Loss: 1.3510... Val Loss 1.3691\n",
            "Epoch: 13/20... Steps: 1690... Loss: 1.3310... Val Loss 1.3688\n",
            "Epoch: 13/20... Steps: 1700... Loss: 1.3310... Val Loss 1.3686\n",
            "Epoch: 13/20... Steps: 1710... Loss: 1.2992... Val Loss 1.3704\n",
            "Epoch: 13/20... Steps: 1720... Loss: 1.3252... Val Loss 1.3700\n",
            "Epoch: 13/20... Steps: 1730... Loss: 1.3548... Val Loss 1.3682\n",
            "Epoch: 13/20... Steps: 1740... Loss: 1.3289... Val Loss 1.3654\n",
            "Epoch: 13/20... Steps: 1750... Loss: 1.2939... Val Loss 1.3702\n",
            "Epoch: 13/20... Steps: 1760... Loss: 1.3189... Val Loss 1.3606\n",
            "Epoch: 13/20... Steps: 1770... Loss: 1.3339... Val Loss 1.3629\n",
            "Epoch: 13/20... Steps: 1780... Loss: 1.3198... Val Loss 1.3648\n",
            "Epoch: 13/20... Steps: 1790... Loss: 1.3028... Val Loss 1.3624\n",
            "Epoch: 13/20... Steps: 1800... Loss: 1.3306... Val Loss 1.3617\n",
            "Epoch: 14/20... Steps: 1810... Loss: 1.3233... Val Loss 1.3588\n",
            "Epoch: 14/20... Steps: 1820... Loss: 1.3125... Val Loss 1.3538\n",
            "Epoch: 14/20... Steps: 1830... Loss: 1.3359... Val Loss 1.3545\n",
            "Epoch: 14/20... Steps: 1840... Loss: 1.2848... Val Loss 1.3555\n",
            "Epoch: 14/20... Steps: 1850... Loss: 1.2646... Val Loss 1.3552\n",
            "Epoch: 14/20... Steps: 1860... Loss: 1.3252... Val Loss 1.3555\n",
            "Epoch: 14/20... Steps: 1870... Loss: 1.3262... Val Loss 1.3535\n",
            "Epoch: 14/20... Steps: 1880... Loss: 1.3156... Val Loss 1.3507\n",
            "Epoch: 14/20... Steps: 1890... Loss: 1.3347... Val Loss 1.3523\n",
            "Epoch: 14/20... Steps: 1900... Loss: 1.3121... Val Loss 1.3470\n",
            "Epoch: 14/20... Steps: 1910... Loss: 1.3056... Val Loss 1.3469\n",
            "Epoch: 14/20... Steps: 1920... Loss: 1.3025... Val Loss 1.3481\n",
            "Epoch: 14/20... Steps: 1930... Loss: 1.2691... Val Loss 1.3454\n",
            "Epoch: 14/20... Steps: 1940... Loss: 1.3306... Val Loss 1.3478\n",
            "Epoch: 15/20... Steps: 1950... Loss: 1.3053... Val Loss 1.3458\n",
            "Epoch: 15/20... Steps: 1960... Loss: 1.3014... Val Loss 1.3413\n",
            "Epoch: 15/20... Steps: 1970... Loss: 1.3022... Val Loss 1.3390\n",
            "Epoch: 15/20... Steps: 1980... Loss: 1.2881... Val Loss 1.3415\n",
            "Epoch: 15/20... Steps: 1990... Loss: 1.2956... Val Loss 1.3461\n",
            "Epoch: 15/20... Steps: 2000... Loss: 1.2711... Val Loss 1.3431\n",
            "Epoch: 15/20... Steps: 2010... Loss: 1.2856... Val Loss 1.3413\n",
            "Epoch: 15/20... Steps: 2020... Loss: 1.3162... Val Loss 1.3408\n",
            "Epoch: 15/20... Steps: 2030... Loss: 1.2822... Val Loss 1.3396\n",
            "Epoch: 15/20... Steps: 2040... Loss: 1.2995... Val Loss 1.3363\n",
            "Epoch: 15/20... Steps: 2050... Loss: 1.2744... Val Loss 1.3372\n",
            "Epoch: 15/20... Steps: 2060... Loss: 1.2924... Val Loss 1.3341\n",
            "Epoch: 15/20... Steps: 2070... Loss: 1.2901... Val Loss 1.3345\n",
            "Epoch: 15/20... Steps: 2080... Loss: 1.2876... Val Loss 1.3367\n",
            "Epoch: 16/20... Steps: 2090... Loss: 1.2920... Val Loss 1.3361\n",
            "Epoch: 16/20... Steps: 2100... Loss: 1.2754... Val Loss 1.3305\n",
            "Epoch: 16/20... Steps: 2110... Loss: 1.2687... Val Loss 1.3301\n",
            "Epoch: 16/20... Steps: 2120... Loss: 1.2734... Val Loss 1.3326\n",
            "Epoch: 16/20... Steps: 2130... Loss: 1.2560... Val Loss 1.3301\n",
            "Epoch: 16/20... Steps: 2140... Loss: 1.2676... Val Loss 1.3312\n",
            "Epoch: 16/20... Steps: 2150... Loss: 1.2770... Val Loss 1.3309\n",
            "Epoch: 16/20... Steps: 2160... Loss: 1.2691... Val Loss 1.3304\n",
            "Epoch: 16/20... Steps: 2170... Loss: 1.2658... Val Loss 1.3309\n",
            "Epoch: 16/20... Steps: 2180... Loss: 1.2655... Val Loss 1.3259\n",
            "Epoch: 16/20... Steps: 2190... Loss: 1.2856... Val Loss 1.3282\n",
            "Epoch: 16/20... Steps: 2200... Loss: 1.2665... Val Loss 1.3192\n",
            "Epoch: 16/20... Steps: 2210... Loss: 1.2398... Val Loss 1.3264\n",
            "Epoch: 16/20... Steps: 2220... Loss: 1.2800... Val Loss 1.3282\n",
            "Epoch: 17/20... Steps: 2230... Loss: 1.2529... Val Loss 1.3201\n",
            "Epoch: 17/20... Steps: 2240... Loss: 1.2658... Val Loss 1.3211\n",
            "Epoch: 17/20... Steps: 2250... Loss: 1.2473... Val Loss 1.3256\n",
            "Epoch: 17/20... Steps: 2260... Loss: 1.2535... Val Loss 1.3227\n",
            "Epoch: 17/20... Steps: 2270... Loss: 1.2578... Val Loss 1.3204\n",
            "Epoch: 17/20... Steps: 2280... Loss: 1.2623... Val Loss 1.3219\n",
            "Epoch: 17/20... Steps: 2290... Loss: 1.2681... Val Loss 1.3216\n",
            "Epoch: 17/20... Steps: 2300... Loss: 1.2282... Val Loss 1.3196\n",
            "Epoch: 17/20... Steps: 2310... Loss: 1.2589... Val Loss 1.3169\n",
            "Epoch: 17/20... Steps: 2320... Loss: 1.2480... Val Loss 1.3120\n",
            "Epoch: 17/20... Steps: 2330... Loss: 1.2442... Val Loss 1.3178\n",
            "Epoch: 17/20... Steps: 2340... Loss: 1.2559... Val Loss 1.3107\n",
            "Epoch: 17/20... Steps: 2350... Loss: 1.2714... Val Loss 1.3130\n",
            "Epoch: 17/20... Steps: 2360... Loss: 1.2645... Val Loss 1.3121\n",
            "Epoch: 18/20... Steps: 2370... Loss: 1.2333... Val Loss 1.3098\n",
            "Epoch: 18/20... Steps: 2380... Loss: 1.2403... Val Loss 1.3020\n",
            "Epoch: 18/20... Steps: 2390... Loss: 1.2420... Val Loss 1.3076\n",
            "Epoch: 18/20... Steps: 2400... Loss: 1.2728... Val Loss 1.3077\n",
            "Epoch: 18/20... Steps: 2410... Loss: 1.2573... Val Loss 1.3068\n",
            "Epoch: 18/20... Steps: 2420... Loss: 1.2427... Val Loss 1.3032\n",
            "Epoch: 18/20... Steps: 2430... Loss: 1.2467... Val Loss 1.3042\n",
            "Epoch: 18/20... Steps: 2440... Loss: 1.2340... Val Loss 1.2996\n",
            "Epoch: 18/20... Steps: 2450... Loss: 1.2176... Val Loss 1.2973\n",
            "Epoch: 18/20... Steps: 2460... Loss: 1.2486... Val Loss 1.2978\n",
            "Epoch: 18/20... Steps: 2470... Loss: 1.2325... Val Loss 1.3051\n",
            "Epoch: 18/20... Steps: 2480... Loss: 1.2279... Val Loss 1.2973\n",
            "Epoch: 18/20... Steps: 2490... Loss: 1.2219... Val Loss 1.2930\n",
            "Epoch: 18/20... Steps: 2500... Loss: 1.2181... Val Loss 1.2962\n",
            "Epoch: 19/20... Steps: 2510... Loss: 1.2275... Val Loss 1.2953\n",
            "Epoch: 19/20... Steps: 2520... Loss: 1.2461... Val Loss 1.2935\n",
            "Epoch: 19/20... Steps: 2530... Loss: 1.2455... Val Loss 1.2930\n",
            "Epoch: 19/20... Steps: 2540... Loss: 1.2581... Val Loss 1.2938\n",
            "Epoch: 19/20... Steps: 2550... Loss: 1.2211... Val Loss 1.2931\n",
            "Epoch: 19/20... Steps: 2560... Loss: 1.2329... Val Loss 1.2920\n",
            "Epoch: 19/20... Steps: 2570... Loss: 1.2232... Val Loss 1.2942\n",
            "Epoch: 19/20... Steps: 2580... Loss: 1.2512... Val Loss 1.2916\n",
            "Epoch: 19/20... Steps: 2590... Loss: 1.2107... Val Loss 1.2888\n",
            "Epoch: 19/20... Steps: 2600... Loss: 1.2009... Val Loss 1.2889\n",
            "Epoch: 19/20... Steps: 2610... Loss: 1.2246... Val Loss 1.2893\n",
            "Epoch: 19/20... Steps: 2620... Loss: 1.2042... Val Loss 1.2837\n",
            "Epoch: 19/20... Steps: 2630... Loss: 1.2064... Val Loss 1.2854\n",
            "Epoch: 19/20... Steps: 2640... Loss: 1.2241... Val Loss 1.2871\n",
            "Epoch: 20/20... Steps: 2650... Loss: 1.2195... Val Loss 1.2868\n",
            "Epoch: 20/20... Steps: 2660... Loss: 1.2316... Val Loss 1.2825\n",
            "Epoch: 20/20... Steps: 2670... Loss: 1.2382... Val Loss 1.2853\n",
            "Epoch: 20/20... Steps: 2680... Loss: 1.2248... Val Loss 1.2855\n",
            "Epoch: 20/20... Steps: 2690... Loss: 1.2070... Val Loss 1.2852\n",
            "Epoch: 20/20... Steps: 2700... Loss: 1.2229... Val Loss 1.2822\n",
            "Epoch: 20/20... Steps: 2710... Loss: 1.1910... Val Loss 1.2853\n",
            "Epoch: 20/20... Steps: 2720... Loss: 1.1954... Val Loss 1.2812\n",
            "Epoch: 20/20... Steps: 2730... Loss: 1.1896... Val Loss 1.2762\n",
            "Epoch: 20/20... Steps: 2740... Loss: 1.1880... Val Loss 1.2826\n",
            "Epoch: 20/20... Steps: 2750... Loss: 1.1982... Val Loss 1.2804\n",
            "Epoch: 20/20... Steps: 2760... Loss: 1.1874... Val Loss 1.2780\n",
            "Epoch: 20/20... Steps: 2770... Loss: 1.2273... Val Loss 1.2820\n",
            "Epoch: 20/20... Steps: 2780... Loss: 1.2486... Val Loss 1.2820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZMHYnEzQIibl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'rnn_x_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kHQG1GP_Imbs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Making predictions\n",
        "\n",
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hyffuvo2Iuyu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Priming and generating text\n",
        "\n",
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rJIIJQAPI2w8",
        "colab_type": "code",
        "outputId": "b1b6fe15-fd2e-4c43-e79b-59a4b9a78509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna, and the letter,\n",
            "he felt this to the children, and with a strange times had been too talking about\n",
            "the same to his brother, the study of those coming to her from the serive that was the\n",
            "strength that his brilliante and an officer who had threw them, and had begun in the same\n",
            "starching and the memory, they arrived. He stood all the simple of\n",
            "happiness, and the frown of the same amount of all the sareness of the\n",
            "conslituse of anything briefly already from him to suppose what she was told any\n",
            "one of the children. She was sitting over to the stall to this minute.\n",
            "\n",
            "\"What's all strong, as It were to me? You know your second some only three at the\n",
            "candle of a passion of such cander is sorty to make their position. I'll\n",
            "speak to you,\" said Vronsky. \"Where you might supper? Thenen your\n",
            "silence is the conversation was so in the mards, that you, to be for\n",
            "me to see, I can go in. I've struck you, and so much a staped. I don't say that the\n",
            "prince he was in her men. I'm a glad with her for answer, a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6DvLFnFyI5Rb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open('rnn_x_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f-MWlybPJCvN",
        "colab_type": "code",
        "outputId": "8b8e0a91-bb90-4bc0-f2ff-1cb421175186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "cell_type": "code",
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And Levin said:\n",
            "\n",
            "Her close had been too have a married the strong way. All on the figure of her face\n",
            "than she was still far importance to herself it seemed so as it was a latter and\n",
            "wanted.\n",
            "\n",
            "\"I'll come to see you,\" she said, and stricking all the summer.\n",
            "\n",
            "\"I have taken a little and servow, there was the papirs, the particular as the\n",
            "servant, and shook home, and then the moning is that it's needed in her his\n",
            "subject, it arrived with their children.... A man in that mistake of\n",
            "my worl who has a country, as it was something, I should have asked you\n",
            "out of a station to time to have been must.\"\n",
            "\n",
            "\"You see through the simple to me.\"\n",
            "\n",
            "\"I have not the close to her to be a love of more sid to this.\"\n",
            "\n",
            "\"Well, were what is he took to the presence, but you were saying. It, as you to\n",
            "speak and take a mother,\"\n",
            "he said, and hiding her sorts and her husband's\n",
            "classical peasants, smiling.\n",
            "\n",
            "\"You're such most fining-it, I spoke in her, I, after the same to her. Then, to step her, there was\n",
            "some since this work.\"\n",
            "\n",
            "\"I'm glad you're not finished you, though it is, as it seems there is the portrait of the\n",
            "marsh?\" thought his wealth was he said, with a smile. \"That you've been making the\n",
            "steps. There's all that is all the money on his much table.\"\n",
            "\n",
            "\"You must shout your husband's love he has been mush? A happiness about\n",
            "the same subject, said anything, that's all to be a sudden of his family. Where have\n",
            "your property for me to see how her might take terrous and money,\" said Alexey\n",
            "Alexandrovitch's friendlong, wearing his hand to happy to her, but the last time so though she\n",
            "heard the princess walked in his shoulders. \"You must say this\n",
            "to tell you that to her, and while the courary terrible would\n",
            "be so all,\" he said, learing over the conversation watching his watch her face.\n",
            "\n",
            "\"Yight you should never say anything is so much that I don't she had to.... I want to\n",
            "be to be,\" said Stepan Arkadyevitch.\n",
            "\n",
            "\"The personal parts too's confidence already about her to buy you to be it should\n",
            "be the minute that you have b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FzY-Agd1JE-g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}